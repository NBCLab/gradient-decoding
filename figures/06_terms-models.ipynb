{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from glob import glob\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# from utils import plot_gradient, plot_subcortical_gradient, plot_meta_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = op.abspath(\"../results\")\n",
    "data_dir = op.abspath(\"../data\")\n",
    "output_dir = op.join(result_dir, \"decoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices to show corresponding to auditory, motor, and visual maps\n",
    "term_ns_idxs = np.array([272, 1863, 3144])\n",
    "term_nq_idxs = np.array([468, 3407, 5977])\n",
    "lda_ns_idxs = np.array([116, 92, 62])\n",
    "lda_nq_idxs = np.array([5, 190, 182])\n",
    "gclda_ns_idxs = np.array([95, 13, 54])\n",
    "gclda_nq_idxs = np.array([76, 181, 134])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting.cm import _cmap_d as nilearn_cmaps\n",
    "cmap = nilearn_cmaps['cold_hot']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurosynth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ns_decoder_fn = op.join(output_dir, f\"term_neurosynth_decoder.pkl.gz\")\n",
    "plot_meta_maps(term_ns_decoder_fn, term_ns_idxs, out_dir=\"./Fig/meta-analysis/term_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ns_grad_path = op.join(output_dir, \"term_neurosynth_fslr\")\n",
    "term_ns_grad_lh_fnames = sorted(glob(op.join(term_ns_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "term_ns_grad_rh_fnames = sorted(glob(op.join(term_ns_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "term_ns_grad_fnames = zip(np.array(term_ns_grad_lh_fnames)[term_ns_idxs], np.array(term_ns_grad_rh_fnames)[term_ns_idxs])\n",
    "features_to_plot = [\"auditory\", \"motor\", \"visual\"]\n",
    "\n",
    "plot_gradient(\"../data\", term_ns_grad_fnames, features_to_plot, threshold_=2, title=True, cmap=cmap, out_dir=\"./Fig/meta-analysis/term_ns\", prefix=\"fsLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surfplot.utils import add_fslr_medial_wall\n",
    "term_ns_idxs = np.array([272, 1863, 3144])\n",
    "gclda_nq_idxs = np.array([76, 181, 134])\n",
    "\n",
    "arr_ = np.load(\"/Users/jperaza/Desktop/tes_fetcher/new/gclda_neuroquery_metamaps.npz\")[\"arr\"]\n",
    "#df = pd.read_csv(\"/Users/jperaza/Desktop/tes_fetcher/term_neurosynth_features.csv\")\n",
    "#features = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr_ = np.load(\"/Users/jperaza/Desktop/tes_fetcher/new/gclda_neuroquery_metamaps.npz\")[\"arr\"]\n",
    "old_arr_ = np.load(\"/Users/jperaza/Desktop/tes_fetcher/decoding/gclda_neuroquery_metamaps.npz\")[\"arr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "assert_array_equal(new_arr_, old_arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromaps.datasets import fetch_fslr\n",
    "from surfplot.utils import threshold\n",
    "\n",
    "neuromaps_dir = op.join(data_dir, \"neuromaps-data\")\n",
    "surfaces = fetch_fslr(density=\"32k\", data_dir=neuromaps_dir)\n",
    "\n",
    "lh, rh = surfaces[\"inflated\"]\n",
    "sulc_lh, sulc_rh = surfaces[\"sulc\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_ns_decoder_fn = op.join(output_dir, f\"lda_neurosynth_decoder.pkl.gz\")\n",
    "plot_meta_maps(lda_ns_decoder_fn, lda_ns_idxs, out_dir=\"./Fig/meta-analysis/lda_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_ns_grad_path = op.join(output_dir, \"lda_neurosynth_fslr\")\n",
    "lda_ns_grad_lh_fnames = sorted(glob(op.join(lda_ns_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "lda_ns_grad_rh_fnames = sorted(glob(op.join(lda_ns_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "lda_ns_grad_fnames = zip(np.array(lda_ns_grad_lh_fnames)[lda_ns_idxs], np.array(lda_ns_grad_rh_fnames)[lda_ns_idxs])\n",
    "lda_features_to_plot = [\"117_auditory_modality_visual\", \"93_motor_premotor_premotor cortex\", \"63_visual_visual cortex_stimuli\"]\n",
    "\n",
    "plot_gradient(\"../data\", lda_ns_grad_fnames, lda_features_to_plot, threshold_=2, title=True, cmap=cmap, out_dir=\"./Fig/meta-analysis/lda_ns\", prefix=\"fsLR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCLDA-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclda_ns_decoder_fn = op.join(output_dir, f\"gclda_neurosynth_model.pkl.gz\")\n",
    "plot_meta_maps(gclda_ns_decoder_fn, gclda_ns_idxs, threshold=0.00001, model=\"gclda\", colorbar=False, out_dir=\"./Fig/meta-analysis/gclda_ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclda_ns_grad_path = op.join(output_dir, \"gclda_neurosynth_fslr\")\n",
    "gclda_ns_grad_lh_fnames = sorted(glob(op.join(gclda_ns_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "gclda_ns_grad_rh_fnames = sorted(glob(op.join(gclda_ns_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "gclda_ns_grad_fnames = zip(np.array(gclda_ns_grad_lh_fnames)[gclda_ns_idxs], np.array(gclda_ns_grad_rh_fnames)[gclda_ns_idxs])\n",
    "gclda_features_to_plot = [\"96_auditory_sounds_sound\", \"14_motor_movements_hand\", \"55_visual_stimuli_color\"]\n",
    "\n",
    "plot_gradient(\"../data\", gclda_ns_grad_fnames, gclda_features_to_plot, threshold_=0.00001, cmap=\"afmhot\", title=True, out_dir=\"./Fig/meta-analysis/gclda_ns\", prefix=\"fsLR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_nq_decoder_fn = op.join(output_dir, f\"term_neuroquery_decoder.pkl.gz\")\n",
    "plot_meta_maps(term_nq_decoder_fn, term_nq_idxs, out_dir=\"./Fig/meta-analysis/term_nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_nq_grad_path = op.join(output_dir, \"term_neuroquery_fslr\")\n",
    "term_nq_grad_lh_fnames = sorted(glob(op.join(term_nq_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "term_nq_grad_rh_fnames = sorted(glob(op.join(term_nq_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "term_nq_grad_fnames = zip(np.array(term_nq_grad_lh_fnames)[term_nq_idxs], np.array(term_nq_grad_rh_fnames)[term_nq_idxs])\n",
    "features_to_plot = [\"auditory\", \"motor\", \"visual\"]\n",
    "\n",
    "plot_gradient(\"../data\", term_nq_grad_fnames, features_to_plot, threshold_=2, title=True, cmap=cmap, out_dir=\"./Fig/meta-analysis/term_nq\", prefix=\"fsLR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_nq_decoder_fn = op.join(output_dir, f\"lda_neuroquery_decoder.pkl.gz\")\n",
    "plot_meta_maps(lda_nq_decoder_fn, lda_nq_idxs, out_dir=\"./Fig/meta-analysis/lda_nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_nq_grad_path = op.join(output_dir, \"lda_neuroquery_fslr\")\n",
    "lda_nq_grad_lh_fnames = sorted(glob(op.join(lda_nq_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "lda_nq_grad_rh_fnames = sorted(glob(op.join(lda_nq_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "lda_nq_grad_fnames = zip(np.array(lda_nq_grad_lh_fnames)[lda_nq_idxs], np.array(lda_nq_grad_rh_fnames)[lda_nq_idxs])\n",
    "lda_features_to_plot = [\"6_sound_auditory_tone\", \"191_motor_sma_m1\", \"183_visual_fixation_orientation\"]\n",
    "\n",
    "plot_gradient(\"../data\", lda_nq_grad_fnames, lda_features_to_plot, threshold_=2, title=True, cmap=cmap, out_dir=\"./Fig/meta-analysis/lda_nq\", prefix=\"fsLR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCLDA-based Meta-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclda_nq_decoder_fn = op.join(output_dir, f\"gclda_neuroquery_model.pkl.gz\")\n",
    "plot_meta_maps(gclda_nq_decoder_fn, gclda_nq_idxs, threshold=0.00001, model=\"gclda\", colorbar=False, out_dir=\"./Fig/meta-analysis/gclda_nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gclda_nq_grad_path = op.join(output_dir, \"gclda_neuroquery_fslr\")\n",
    "gclda_nq_grad_lh_fnames = sorted(glob(op.join(gclda_nq_grad_path, \"*hemi-L_feature.func.gii\")))\n",
    "gclda_nq_grad_rh_fnames = sorted(glob(op.join(gclda_nq_grad_path, \"*hemi-R_feature.func.gii\")))\n",
    "gclda_nq_grad_fnames = zip(np.array(gclda_nq_grad_lh_fnames)[gclda_nq_idxs], np.array(gclda_nq_grad_rh_fnames)[gclda_nq_idxs])\n",
    "gclda_features_to_plot = [\"77_response_sound_model\", \"182_movement_task_motor\", \"135_visual_response_stimuli\"]\n",
    "\n",
    "plot_gradient(\"../data\", gclda_nq_grad_fnames, gclda_features_to_plot, threshold_=0.00001, cmap=\"afmhot\", title=True, out_dir=\"./Fig/meta-analysis/gclda_nq\", prefix=\"fsLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surfplot import Plot\n",
    "full_vertices = 64984\n",
    "hemi_vertices = full_vertices // 2\n",
    "\n",
    "for term_ns_idx in gclda_nq_idxs:\n",
    "    data = arr_[term_ns_idx, :]\n",
    "    data = add_fslr_medial_wall(data)\n",
    "    data_lh, data_rh = data[:hemi_vertices], data[hemi_vertices:full_vertices]\n",
    "    assert_almost_equal(data_lh, data_rh)\n",
    "    data_lh = threshold(data_lh, 0.00001)\n",
    "    data_rh = threshold(data_rh, 0.00001)\n",
    "    \n",
    "    p = Plot(lh, rh)\n",
    "    p.add_layer({'left': sulc_lh, 'right': sulc_rh}, cmap='binary_r', cbar=False)\n",
    "    p.add_layer({'left': data_lh, 'right': data_rh}, cmap=\"afmhot\")\n",
    "    fig = p.build()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import (\n",
    "    assert_allclose,\n",
    "    assert_almost_equal,\n",
    "    assert_array_equal,\n",
    "    assert_equal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_[term_ns_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromaps.datasets import fetch_atlas\n",
    "from nibabel.gifti import GiftiDataArray\n",
    "\n",
    "def zero_fslr_medial_wall(data_lh, data_rh, neuromaps_dir):\n",
    "    \"\"\"Remove medial wall from data in fsLR space\"\"\"\n",
    "\n",
    "    atlas = fetch_atlas(\"fsLR\", \"32k\", data_dir=neuromaps_dir, verbose=0)\n",
    "    medial_lh, medial_rh = atlas[\"medial\"]\n",
    "    medial_arr_lh = nib.load(medial_lh).agg_data()\n",
    "    medial_arr_rh = nib.load(medial_rh).agg_data()\n",
    "\n",
    "    data_arr_lh = data_lh.agg_data()\n",
    "    data_arr_rh = data_rh.agg_data()\n",
    "    data_arr_lh[np.where(medial_arr_lh == 0)] = 0\n",
    "    data_arr_rh[np.where(medial_arr_rh == 0)] = 0\n",
    "\n",
    "    data_lh.remove_gifti_data_array(0)\n",
    "    data_rh.remove_gifti_data_array(0)\n",
    "    data_lh.add_gifti_data_array(GiftiDataArray(data_arr_lh))\n",
    "    data_rh.add_gifti_data_array(GiftiDataArray(data_arr_rh))\n",
    "\n",
    "    return data_lh, data_rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_fslr_medial_wall(data_lh, data_rh, neuromaps_dir, join=True):\n",
    "    \"\"\"Remove medial wall from data in fsLR space.\n",
    "\n",
    "    Data in 32k fs_LR space (e.g., Human Connectome Project data) often in\n",
    "    GIFTI format include the medial wall in their data arrays, which results\n",
    "    in a total of 64984 vertices across hemispheres. This function removes\n",
    "    the medial wall vertices to produce a data array with the full 59412 vertices,\n",
    "    which is used to perform functional decoding.\n",
    "\n",
    "    This function was adapted from :func:`surfplot.utils.add_fslr_medial_wall`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Surface vertices. Must have exactly 32492 vertices per hemisphere.\n",
    "    join : bool\n",
    "        Return left and right hemipsheres in the same arrays. Default: True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Vertices with medial wall excluded (59412 vertices total).\n",
    "\n",
    "    ValueError\n",
    "    ------\n",
    "    `data` has the incorrect number of vertices (59412 or 64984 only\n",
    "        accepted)\n",
    "    \"\"\"\n",
    "    assert data_lh.shape[0] == 32492\n",
    "    assert data_rh.shape[0] == 32492\n",
    "\n",
    "    atlas = fetch_atlas(\"fsLR\", \"32k\", data_dir=neuromaps_dir, verbose=0)\n",
    "    medial_lh, medial_rh = atlas[\"medial\"]\n",
    "    wall_lh = nib.load(medial_lh).agg_data()\n",
    "    wall_rh = nib.load(medial_rh).agg_data()\n",
    "\n",
    "    data_lh = data_lh[np.where(wall_lh != 0)]\n",
    "    data_rh = data_rh[np.where(wall_rh != 0)]\n",
    "\n",
    "    if not join:\n",
    "        return data_lh, data_rh\n",
    "\n",
    "    data = np.hstack((data_lh, data_rh))\n",
    "    assert data.shape[0] == 59412\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "from neuromaps import transforms\n",
    "from nilearn import masking\n",
    "\n",
    "gclda_model_file = gzip.open(\n",
    "    \"/Users/jperaza/Documents/GitHub/gradient-decoding/results/decoding/old/term_neurosynth_decoder.pkl.gz\", \n",
    "    \"rb\",\n",
    ")\n",
    "decoder = pickle.load(gclda_model_file)\n",
    "\n",
    "metamaps_arr = decoder.images_\n",
    "metamaps = decoder.masker.inverse_transform(metamaps_arr)\n",
    "metamaps_arr = None\n",
    "decoder = None\n",
    "\n",
    "# metamaps_arr = decoder.p_topic_g_voxel_.T\n",
    "# metamaps = masking.unmask(metamaps_arr, decoder.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_maps = metamaps.shape[3]\n",
    "\n",
    "metamap_fslr = np.zeros((n_maps, 59412))\n",
    "for map_i in range(n_maps):\n",
    "    metamap = image.index_img(metamaps, map_i)\n",
    "\n",
    "    metamap_lh, metamap_rh = transforms.mni152_to_fslr(metamap, fslr_density=\"32k\")\n",
    "    metamap_lh, metamap_rh = zero_fslr_medial_wall(metamap_lh, metamap_rh, neuromaps_dir)\n",
    "    metamap_arr_lh = metamap_lh.agg_data()\n",
    "    metamap_arr_rh = metamap_rh.agg_data()\n",
    "    metamap_fslr[map_i, :] = rm_fslr_medial_wall(metamap_arr_lh, metamap_arr_rh, neuromaps_dir)\n",
    "\n",
    "np.savez_compressed(\n",
    "    \"/Users/jperaza/Documents/GitHub/gradient-decoding/data/decoding/term_neurosynth_metamaps\",\n",
    "    arr=metamap_fslr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr_ = np.load(\"/Users/jperaza/Documents/GitHub/gradient-decoding/data/decoding/lda_neurosynth_metamaps.npz\")[\"arr\"]\n",
    "old_arr_ = np.load(\"/Users/jperaza/Desktop/tes_fetcher/new/lda_neurosynth_metamaps.npz\")[\"arr\"]\n",
    "#old_arr_ = np.load(\"/Users/jperaza/Desktop/tes_fetcher/decoding/lda_neurosynth_metamaps.npz\")[\"arr\"]\n",
    "assert_array_equal(new_arr_, old_arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_ns_idxs = np.array([116, 92, 62])\n",
    "for map_i in lda_ns_idxs:\n",
    "    data = new_arr_[map_i, :]\n",
    "    data = add_fslr_medial_wall(data)\n",
    "    data_lh, data_rh = data[:hemi_vertices], data[hemi_vertices:full_vertices]\n",
    "    \n",
    "    metamap = image.index_img(metamaps, map_i)\n",
    "    meta_map_lh, meta_map_rh = transforms.mni152_to_fslr(metamap)\n",
    "\n",
    "    meta_map_lh, meta_map_rh = zero_fslr_medial_wall(meta_map_lh, meta_map_rh, neuromaps_dir)\n",
    "    meta_map_arr_lh, meta_map_arr_rh = meta_map_lh.agg_data(), meta_map_rh.agg_data()\n",
    "\n",
    "    # assert_array_equal(meta_map_arr_lh, data_lh)\n",
    "    # assert_array_equal(meta_map_arr_rh, data_rh)\n",
    "    meta_map_arr_lh = threshold(meta_map_arr_lh, 2)\n",
    "    meta_map_arr_rh = threshold(meta_map_arr_rh, 2)\n",
    "\n",
    "    data_lh = threshold(data_lh, 2)\n",
    "    data_rh = threshold(data_rh, 2)\n",
    "    \n",
    "    p = Plot(lh, rh)\n",
    "    p.add_layer({'left': sulc_lh, 'right': sulc_rh}, cmap='binary_r', cbar=False)\n",
    "    p.add_layer({'left': data_lh, 'right': data_rh}, cmap=cmap)\n",
    "    fig = p.build()\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gradec_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32eae8b81dca7564140c4bb02978cc1f5cb3ca50d75c604cb4f67ffb4db99fb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
